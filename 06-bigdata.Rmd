# Big data {#bigdata}

## How to deal with (very / too) large datasets?

1. Use more RAM / processors / drive space...
2. Use less data: (re)sample, ...
3. Use a database
4. Use specific R packages (`ff`, `bigmemory`)
5. Use other tools

## How big is big?

1. Fits in RAM and on drive (but slow)
2. Doesn't fit in RAM but fits on drive
3. Doesn't fit in RAM and doesn't fit on drive

## List of tools

Reading: @Varian2014 ([PDF available](http://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.2.3))

```{r echo=FALSE, fig.align="center", tool_list, fig.cap=""}
knitr::include_graphics("images/ch6_tool_list.png")
```

Spark?  h2o?  More?  Let's go back to the bottlenecks

- CPU
- RAM
- I/O

## Data that fits in memory

### Faster I/O

Reading: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html

`data.table` provides an enhanced of a `data.frame` and faster I/O with `fread` and `fwrite`.

To read the 0.5GB ratings file from MovieLens

```{r eval=FALSE}
library(data.table)
system.time(ratings <- fread("~/Dropbox/Data17/ml-20m/ratings.csv"))
```

takes

```{txt}
Read 20000263 rows and 4 (of 4) columns from 0.497 GB file in 00:00:05
   user  system elapsed 
  4.007   0.229   4.244
```

while

```{r eval=FALSE}
system.time(ratings <- read.csv("~/Dropbox/Data17/ml-20m/ratings.csv"))
```

takes

```{txt}
   user  system elapsed 
 85.199   2.711  90.997 
```

There are ways to improve the speed of `read.csv` (for example, but specifying column types).  But in general `fread` is much faster.

```{r eval=FALSE}
library(readr) # in tidyverse
system.time(ratings <- read_csv("~/Dropbox/Data17/ml-20m/ratings.csv"))
```

```{txt}
   user  system elapsed 
 10.290   3.037  18.450 
```

also tends to perform better than `read.csv`.

```{r echo = FALSE, results = 'asis'}
library(knitr)
tools <- data.frame("package" = c("base", "data.table", "readr"),
                    "function" = c("read.csv", "fread", "read_csv"),
                    "speed" = c("slow", "very fast", "fast"),
                    "output" = c("data.frame", "data.table", "tibble"))
kable(tools, caption = "I/O comparison")
```

### Reference vs copy

## Data that doesn't fit in memory (but fits on drive)

Let's try to work with a 12GB file and 4/8 GB of memory...



## Pure R solutions

### Sampling

### `bigmemory`

https://cran.r-project.org/web/packages/bigmemory/index.html

Reading: https://cran.r-project.org/web/packages/bigmemory/vignettes/Overview.pdf

__bigmemory__: Manage Massive Matrices with Shared Memory and Memory-Mapped Files

Create, store, access, and manipulate massive matrices. Matrices are allocated to shared memory and may use memory-mapped files. Packages 'biganalytics', 'bigtabulate', 'synchronicity', and 'bigalgebra' provide advanced functionality.

__(+)__ pure R solution from a user perspective

__(-)__ mostly for numeric data matrices

## `bigmemory`

```{r eval=FALSE}
library(bigmemory)
library(biganalytics)
library(bigtabulate)
library(biglm)

flights <- read.big.matrix(
  "/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.csv",
  header = TRUE,
  backingfile = "allyears2k.bin",
  backingpath = "/Users/cchoirat/Dropbox/Data17/AirFlights/",
  descriptorfile = "allyears2k.desc",
  shared = TRUE)

f <- attach.big.matrix("/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.desc")
dim(f)
colnames(f)
mean(f[, "ArrDelay"], na.rm = TRUE)

fit <- biglm.big.matrix(ArrDelay ~ Distance, data = f)
summary(fit)
```

```{r eval=FALSE}
library(data.table)
airlines <- fread("/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.csv")
rfit <- lm(ArrDelay ~ Distance, data = airlines)
summary(rfit)
```


### Database connections and lazy evaluation

## Scaling up

### Parallel computing and clusters

### Cloud computing

### Spark

Reading: https://spark.rstudio.com/

```{r eval=FALSE}
library(sparklyr)
spark_install(version = "2.1.0")
```

```{r eval=FALSE}
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "32G"
conf$spark.memory.fraction <- 0.5
sc <- spark_connect(master = "local")
```

```{r eval=FALSE}
library(dplyr)
iris_tbl <- copy_to(sc, iris)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
batting_tbl <- copy_to(sc, Lahman::Batting, "batting")
src_tbls(sc)
```

```{r eval=FALSE}
top_rows <- read.csv("~/Dropbox/Data17/AirFlights/allyears.csv", nrows = 5)
file_columns <- top_rows %>% 
  purrr::map(function(x)"character")
rm(top_rows)
```


```{r eval=FALSE}
sp_flights <- spark_read_csv(sc, 
                             name = "flights2", 
                             path = "~/Dropbox/Data17/AirFlights/allyears.csv", 
                             memory = FALSE, 
                             columns = file_columns, 
                             infer_schema = FALSE)
```

```{r eval=FALSE}
flights_table <- sp_flights %>%
  mutate(DepDelay = as.numeric(DepDelay),
         ArrDelay = as.numeric(ArrDelay),
         SchedDeparture = as.numeric(CRSDepTime)) %>%
  select(Origin, Dest, SchedDeparture, ArrDelay, DepDelay, Month, DayofMonth)

flights_table %>% head
```

Cache data:

```{r eval=FALSE}
sp_flights %>%
  tally # takes a looooong time
```

123534969...

```{r eval=FALSE}
subset_table <- flights_table %>% 
  compute("flights_subset")
```

```{r eval=FALSE}
subset_table %>%
  tally # a bit faster.
```

123534969 as well!

```{r eval=FALSE}
lm(arr_delay ~ distance, data = flights_tbl)
ml_linear_regression(subset_table, response = "ArrDelay", features = "SchedDeparture")
```



TODOL change the `config` arguments of the connection

### `h2o` and `Sparkling Water`

Reading: https://spark.rstudio.com/h2o.html

### More?

GPU
