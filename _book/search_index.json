[
["intro.html", "Computing for Big Data (BST-262) Chapter 1 Introduction 1.1 Logistics 1.2 Prerequisites 1.3 Rationale 1.4 Big data bottlenecks 1.5 Syllabus 1.6 Evaluation 1.7 Software tools and packages 1.8 Datasets 1.9 Contributing with GitHub 1.10 Before we start… 1.11 Style", " Computing for Big Data (BST-262) Christine Choirat 2017-12-10 Chapter 1 Introduction 1.1 Logistics Fall 2 course Tuesday and Thursday, 11:30am-1pm Contact info: Christine Choirat (cchoirat@iq.harvard.edu). Please use BST232 in the email title. TA’s: Qian Di (qiandi@mail.harvard.edu) and Ben Sabath (mbsabath@hsph.harvard.edu) Office hours: Ben: Tuesday 1:30-2:30pm Qian: Thursday 10:30-11:30am Christine: Tuesday 10:30-11:30am (office 437A) Course GitHub repository https://github.com/cchoirat/bigdata17 Open file in folder _book/index.html These course notes are work in progress. 1.2 Prerequisites For BST262 (Computing for Big Data), we assume familiarity with the material covered in BST260 (Introduction to Data Science). We will use R to present concepts that are mostly language-agnostic. We could have used Python, as in BST261 (Data Science II). 1.3 Rationale Available data grows at a much faster rate than available computing capacity. Statistical software programs such as R were not designed to handle datasets of massive size. 1.4 Big data bottlenecks As described by Lim and Tjhi (2015), there are three bottlenecks: CPU RAM I/O Figure 1.1: Steps to execute an R program, from Lim and Tjhi (2015), Chapter 1. Exercise 1.1 Can you identify points 1–7 in the following code snippet? data &lt;- read.csv(&quot;mydata.csv&quot;) totals &lt;- colSums(data) write.csv(totals, &quot;totals.csv&quot;) 1.5 Syllabus Part I – Good code still matters (even with lots of computing resources) Week 1 - Basic tools Lecture 1. Unix scripting, make Lecture 2. Version control: Git and GitHub (guest lecture: Ista Zhan) Week 2 - Creating and maintaining R packages Lecture 3. Rationale, package structure, available tools Lecture 4. Basics of software engineering: unit testing, code coverage, continuous integration Week 3 - Software optimization Lecture 5. Measuring performance: profiling and benchmarking tools Lecture 6. Improving performance: an introduction to C/C++, Rcpp Part II – Scaling up (don’t use big data tools for small data) Week 4 – Databases Lecture 7. Overview of SQL (SQLite, PostgreSQL) and noSQL databases (HBase, MongoDB, Cassandra, BigTable, …) Lecture 8. R database interfaces (in particular through dplyr and mongolite) Week 5 - Analyzing data that does not fit in memory Lecture 9. Pure R solutions (sampling, ff and bigmemory, other interpreters). JVM solutions (h20, Spark) Lecture 10. An introduction to parallel computing; clusters and cloud computing. “Divide and Conquer” (MapReduce approaches) Week 6 – Visualization Lecture 11. Principles of visualization (guest lecture: James Honaker) Lecture 12. Maps and GIS: principles of GIS, using R as a GIS, PostGIS Weeks 7 &amp; 8 - Guest lectures (order and precise schedule TBD) Software project management (Danny Brooke) R and Spark (Ellen Kraffmiller and Robert Treacy) Advanced GIS and remote sensing (TBD) Cluster architecture (William J. Horka) 1.6 Evaluation Grades will be based on two mandatory problem sets. Each problem set will correspond to 50% (= 50 points) of the final grade. The first problem set will be available by the end of week 3 and the second problem set by the end of week 6. You will be required to submit problem set solutions within two weeks. Grades, and feedback when appropriate, will be returned two weeks after submission. You will submit a markdown document that combines commented code for data analysis and detailed and structured explanations of the algorithms and software tools that you used. 1.7 Software tools and packages We will mostly use R in this course. Some examples will be run in Python. In general, we will use free and open-source software programs such as PostgreSQL / PostGIS or Spark. 1.8 Datasets We have collected datasets to illustrate concepts. They are hosted on a Dropbox folder. 1.8.1 MovieLens MovieLens by Harper and Konstan (2015, https://grouplens.org/datasets/movielens/) collects datasets from the website https://movielens.org/. There are datasets of different sizes. We will use: Small (1MB): https://grouplens.org/datasets/movielens/latest/ Benchmark (~190MB zipped): https://grouplens.org/datasets/movielens/20m/ 1.8.2 Airlines data The airlines dataset comes from the U.S. Department of Transportation and were used in the 2009 Data Expo of the American Statistical Association (ASA). We will use a version curated by h2o: https://github.com/h2oai/h2o-2/wiki/Hacking-Airline-DataSet-with-H2O. 1.8.3 Insurance claims Claims data contain Protected Health Information (PHI). There are strong privacy restrictions to store, use and share this type of data. We will use synthetic data (Sample 1) from the Centers for Medicare and Medicaid Services (CMS). 1.8.4 Census Census data is commonly merged with administrative claims data such as Medicare. We will use data from the Census Bureau. 1.8.5 PM2.5 exposure We will use PM2.5 exposure data from the EPA Air Quality System (AQS) to illustrate GIS linkage concepts. 1.8.6 Methylation If there is enough interest, we might present methylation examples. 1.9 Contributing with GitHub If you have suggestions, you can open a GitHub issue at https://github.com/cchoirat/bigdata17/issues. If you want to contribute, we welcome pull requests. 1.10 Before we start… How much R do you know? Introduction to R: http://tutorials.iq.harvard.edu/R/Rintro/Rintro.html Regression models in R: http://tutorials.iq.harvard.edu/R/Rstatistics/Rstatistics.html R graphics: http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html R programming: http://tutorials.iq.harvard.edu/R/RProgramming/Rprogramming.html 1.11 Style Reading: http://adv-r.had.co.nz/Style.html References "],
["basics.html", "Chapter 2 Basic tools 2.1 Command line tools 2.2 Makefiles 2.3 Git and GitHub", " Chapter 2 Basic tools In this Chapter, we present basic tools that will be important when interacting with big data systems: the command-line interface (CLI) in a Unix shell and several utilities (less, awk, vi and make). 2.1 Command line tools We assume some familiarity with the Unix shell, for example as in http://swcarpentry.github.io/shell-novice/. We also assume that you have access to a shell, either because you use Linux or OS X or because you have the right tools on Windows (for example Cygwin or the Bash shell in Windows 10). 2.1.1 Why use the command line? Batch processing Cluster and cloud computing 2.1.2 Basic Unix tools 2.1.3 Useful tools 2.1.3.1 less less is a pager that lets you view one page at a time files that can be very large. File DE1_0_2008_to_2010_Carrier_Claims_Sample_1A.csv in Data17/SyntheticMedicare is 1.2GB. Even if we have enough RAM to process the data, less helps get a very quick sense of the data (variable names, separators, etc.) 2.1.3.2 awk awk is a text-processing programming language available on most Unix systems. It can be used for data extraction. 2.1.3.3 vi vi is a screen-based text editor available on almost all Unix systems. Most versions are actually Vim (that stands for “Vi IMproved”). There are many cheat sheets and tutorials available on-line (for example, the interactive http://www.openvim.com/). I invite you to learn basics vi commands. 2.1.4 Example Let’s apply some of the techniques described in Blackwell and Sen (2012) on Fisher’s Iris data set saved in tab-delimited format. Of course, it is a small dataset easily processed with R: iris &lt;- read.table(&quot;~/Dropbox/Data17/iris/iris.tab&quot;) head(iris, n = 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa In a shell, we can use: head -n 6 ~/Dropbox/Data17/iris/iris.tab ## &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; ## &quot;1&quot; 5.1 3.5 1.4 0.2 &quot;setosa&quot; ## &quot;2&quot; 4.9 3 1.4 0.2 &quot;setosa&quot; ## &quot;3&quot; 4.7 3.2 1.3 0.2 &quot;setosa&quot; ## &quot;4&quot; 4.6 3.1 1.5 0.2 &quot;setosa&quot; ## &quot;5&quot; 5 3.6 1.4 0.2 &quot;setosa&quot; Suppose we only need to select two variables in our model, Sepal.Length and Species. In R, we can use: iris_subset &lt;- iris[, c(&quot;Sepal.Length&quot;, &quot;Species&quot;)] or iris_subset &lt;- iris[, c(1, 5)] head(iris_subset) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa With the tidyverse, we can use pipes. The %&gt;% operator allows for performing chained operations. iris %&gt;% select(1, 5) %&gt;% head() In a shell, the pipe operator to combine shell commands is | and we can use: cut -f 1,5 ~/Dropbox/Data17/iris/iris.tab | head -n 7 ## &quot;Sepal.Length&quot; &quot;Species&quot; ## &quot;1&quot; 0.2 ## &quot;2&quot; 0.2 ## &quot;3&quot; 0.2 ## &quot;4&quot; 0.2 ## &quot;5&quot; 0.2 ## &quot;6&quot; 0.4 To keep observations with “Sepal.Length” greater than 5: iris %&gt;% filter(Sepal.Length &gt; 5) %&gt;% head() In the shell, we can use the AWK programming language. We start from row NR 2 (we could start from row 1, it contains variable names) and select rows such that the second variable (Sepal.Length) is greater than 5. awk &#39;NR == 2 || $2 &gt; 5&#39; ~/Dropbox/Data17/iris/iris.tab | head ## &quot;1&quot; 5.1 3.5 1.4 0.2 &quot;setosa&quot; ## &quot;6&quot; 5.4 3.9 1.7 0.4 &quot;setosa&quot; ## &quot;11&quot; 5.4 3.7 1.5 0.2 &quot;setosa&quot; ## &quot;15&quot; 5.8 4 1.2 0.2 &quot;setosa&quot; ## &quot;16&quot; 5.7 4.4 1.5 0.4 &quot;setosa&quot; ## &quot;17&quot; 5.4 3.9 1.3 0.4 &quot;setosa&quot; ## &quot;18&quot; 5.1 3.5 1.4 0.3 &quot;setosa&quot; ## &quot;19&quot; 5.7 3.8 1.7 0.3 &quot;setosa&quot; ## &quot;20&quot; 5.1 3.8 1.5 0.3 &quot;setosa&quot; ## &quot;21&quot; 5.4 3.4 1.7 0.2 &quot;setosa&quot; Exercise 2.1 The iris dataset is also saved in .csv format at ~/Dropbox/Data17/iris/iris.csv. Use AWK and tail to select the last 5 observations where Sepal.Width is larger than 3.5 and Petal.Length is smaller than 1.5. 2.2 Makefiles make is a tool that helps put all the (interdependent) pieces of an analytic workflow together: data retrieving data cleaning analysis graphs reports … 2.2.1 Simulate data in R set.seed(123) File simulate_data.R # set.seed(123) N &lt;- 1000 # sample size X1 &lt;- rpois(n = N, lambda = 50) X2 &lt;- 10 + rbinom(n = N, prob = 0.8, size = 1) Y &lt;- 10 + 3 * X1 + -5 * X2 + 3 * rnorm(n = N) write.csv(data.frame(Y = Y, X1 = X1, X2 = X2), &quot;sample_data.csv&quot;, row.names = FALSE) head(data.frame(Y = Y, X1 = X1, X2 = X2)) ## Y X1 X2 ## 1 88.74430 46 11 ## 2 125.77081 58 11 ## 3 70.76396 38 10 ## 4 110.32157 50 10 ## 5 145.79546 62 11 ## 6 109.45403 53 11 2.2.2 Create a plot in Python File create_graph.py import pandas as pd import matplotlib.pyplot as plt sim_data = pd.read_csv(&quot;sample_data.csv&quot;) plt.figure() sim_data.plot() plt.savefig(&quot;plot.pdf&quot;, format = &quot;pdf&quot;) 2.2.3 Run statistical model in R We can estimate the model with R: sim_data &lt;- read.csv(&quot;sample_data.csv&quot;) summary(lm(Y ~ X1 + X2, data = sim_data)) ## ## Call: ## lm(formula = Y ~ X1 + X2, data = sim_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.3988 -1.9452 -0.0261 2.0216 9.1066 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.09087 2.54667 3.57 0.000374 *** ## X1 3.00531 0.01326 226.68 &lt; 2e-16 *** ## X2 -4.94658 0.22876 -21.62 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.936 on 997 degrees of freedom ## Multiple R-squared: 0.9811, Adjusted R-squared: 0.981 ## F-statistic: 2.585e+04 on 2 and 997 DF, p-value: &lt; 2.2e-16 2.2.4 Run statistical model in R To save the output, we use the sink function. File estimate_model.R sink(&quot;estimation_summary.txt&quot;) summary(lm(Y ~ X1 + X2, data = sim_data)) sink() 2.2.5 Makefile syntax make is a command that runs on a text file often named Makefile. A Makefile contains one or several blocks with the following structure: targetfile: sourcefile(s) [tab] command 2.2.6 Naive version File: Makefile sample_data.csv: simulate_data.R R CMD BATCH simulate_data.R plot.pdf: create_graph.py python create_graph.py estimation_summary.txt: estimate_model.R R CMD BATCH estimate_model.R A simple call to make only builds the first target (sample_data.csv). To build the other targets, we have to use: make plot.pdf and make estimation_summary.txt. 2.2.7 Making all targets File: Makefile all: analysis analysis: sample_data.csv plot.pdf estimation_summary.txt sample_data.csv: simulate_data.R R CMD BATCH simulate_data.R plot.pdf: create_graph.py python create_graph.py estimation_summary.txt: estimate_model.R R CMD BATCH estimate_model.R New data is simulated and saved in sample_data.csv. But plot.pdf and estimation_summary.txt are not updated. 2.2.8 Dealing with dependencies Problem plot.pdf and estimation_summary.txt depend on sample_data.csv. Solution: explicit dependencies. File: Makefile all: analysis analysis: sample_data.csv plot.pdf estimation_summary.txt sample_data.csv: simulate_data.R R CMD BATCH simulate_data.R plot.pdf: sample_data.csv create_graph.py python create_graph.py estimation_summary.txt: sample_data.csv estimate_model.R R CMD BATCH estimate_model.R 2.3 Git and GitHub Guest lecture by Ista Zahn. References "],
["packages.html", "Chapter 3 Packages 3.1 Why? 3.2 Package structure 3.3 Building steps 3.4 Create an R package 3.5 R packages on GitHub 3.6 RStudio projects 3.7 Package workflow example 3.8 Unit testing 3.9 Continuous integration 3.10 Code coverage 3.11 Back to GitHub 3.12 Vignettes", " Chapter 3 Packages We strongly recommand Wickham (2015). We assume the following packages are installed: install.packages(c(&quot;devtools&quot;, &quot;roxygen2&quot;, &quot;testthat&quot;, &quot;knitr&quot;)) 3.1 Why? Organize your code Distribute your code Keep versions of your code 3.2 Package structure Folder hierarchy NAMESPACE: package import / export DESCRIPTION: metadata R/: R code man/: object documentation (with short examples) tests/ data/ src/: compiled code vignettes/: manual-like documentation inst/: installed files demo/: longer examples exec, po, tools 3.3 Building steps R CMD build R CMD INSTALL R CMD check 3.3.1 R CMD build R CMD build --help Build R packages from package sources in the directories specified by ‘pkgdirs’ 3.3.2 R CMD INSTALL R CMD INSTALL --help Install the add-on packages specified by pkgs. The elements of pkgs can be relative or absolute paths to directories with the package sources, or to gzipped package ‘tar’ archives. The library tree to install to can be specified via ‘–library’. By default, packages are installed in the library tree rooted at the first directory in .libPaths() for an R session run in the current environment. 3.3.3 R CMD check R CMD check --help http://r-pkgs.had.co.nz/check.html Check R packages from package sources, which can be directories or package ‘tar’ archives with extension ‘.tar.gz’, ‘.tar.bz2’, ‘.tar.xz’ or ‘.tgz’. A variety of diagnostic checks on directory structure, index and control files are performed. The package is installed into the log directory and production of the package PDF manual is tested. All examples and tests provided by the package are tested to see if they run successfully. By default code in the vignettes is tested, as is re-building the vignette PDFs. 3.3.4 Building steps with devtools devtools::build devtools::install devtools::check and many others: load_all, document, test, run_examples, … 3.4 Create an R package 3.4.1 utils::package.skeleton package.skeleton() # &quot;in &quot;clean&quot; session (&quot;anRpackage&quot;) package.skeleton(&quot;pkgname&quot;) # in &quot;clean&quot; session set.seed(02138) f &lt;- function(x, y) x+y g &lt;- function(x, y) x-y d &lt;- data.frame(a = 1, b = 2) e &lt;- rnorm(1000) package.skeleton(list = c(&quot;f&quot;,&quot;g&quot;,&quot;d&quot;,&quot;e&quot;), name = &quot;pkgname&quot;) 3.4.2 devtools::create devtools::create(&quot;path/to/package/pkgname&quot;) Also from RStudio (`File -&gt; New Project’). 3.4.3 Submit to CRAN Figure 3.1: Submitting to CRAN. It’s not that bad… Reading: http://r-pkgs.had.co.nz/release.html 3.5 R packages on GitHub Reading: http://r-pkgs.had.co.nz/git.html Version control Website, wiki, project management Easy install: install_github from devtools Collaboration Issue tracking 3.5.0.1 RStudio and GitHub integration Figure 3.2: Create a new Linreg repository on GitHub Figure 3.3: Create a new project in RStudio Figure 3.4: Select R package Figure 3.5: Create the Linreg R package as a Git repository Figure 3.6: Automatically created files Figure 3.7: Build tab in RStudio Figure 3.8: Github webpage Figure 3.9: Open a terminal Command line # git init # already run when creating package with RStudio git add * git commit -m &quot;First commit&quot; git remote add origin https://github.com/cchoirat/Linreg git push -u origin master Figure 3.10: Github webpage is updated 3.5.1 .gitignore RStudio default .Rproj.user .Rhistory .RData GitHub default # History files .Rhistory .Rapp.history # Example code in package build process *-Ex.R # RStudio files .Rproj.user/ # produced vignettes vignettes/*.html vignettes/*.pdf 3.6 RStudio projects .Rproj file extension, in our example Linreg.Rproj A project has its own: R session .Rprofile (e.g., to customize startup environment) .Rhistory Default working directory is project directory Keeps track of project-specific recent files 3.6.1 Project options Version: 1.0 RestoreWorkspace: Default SaveWorkspace: Default AlwaysSaveHistory: Default EnableCodeIndexing: Yes UseSpacesForTab: Yes NumSpacesForTab: 2 Encoding: UTF-8 RnwWeave: knitr LaTeX: pdfLaTeX AutoAppendNewline: Yes StripTrailingWhitespace: Yes BuildType: Package PackageUseDevtools: Yes PackageInstallArgs: --no-multiarch --with-keep.source 3.6.2 Package documentation Functions and methods Vignettes PDF knitr 3.7 Package workflow example Creating R Packages: A Tutorial (Friedrich Leisch, 2009) Our example is adapted from https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf. 3.7.1 Add linreg.R to R/ directory linmodEst &lt;- function(x, y) { ## CC: crossprod or a QR decomposition (as in the original version) are more efficient coef &lt;- solve(t(x) %*% x) %*% t(x) %*% y print(coef) ## degrees of freedom and standard deviation of residuals df &lt;- nrow(x) - ncol(x) sigma2 &lt;- sum((y - x %*% coef) ^ 2) / df ## compute sigma^2 * (x’x)^-1 vcov &lt;- sigma2 * solve(t(x) %*% x) colnames(vcov) &lt;- rownames(vcov) &lt;- colnames(x) list( coefficients = coef, vcov = vcov, sigma = sqrt(sigma2), df = df ) } 3.7.2 Run our function data(cats, package = &quot;MASS&quot;) linmodEst(cbind(1, cats$Bwt), cats$Hwt) ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## $coefficients ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## ## $vcov ## [,1] [,2] ## [1,] 0.4792475 -0.17058197 ## [2,] -0.1705820 0.06263081 ## ## $sigma ## [1] 1.452373 ## ## $df ## [1] 142 We can compare the output with lm. lm1 &lt;- lm(Hwt ~ Bwt, data = cats) lm1 ## ## Call: ## lm(formula = Hwt ~ Bwt, data = cats) ## ## Coefficients: ## (Intercept) Bwt ## -0.3567 4.0341 coef(lm1) ## (Intercept) Bwt ## -0.3566624 4.0340627 vcov(lm1) ## (Intercept) Bwt ## (Intercept) 0.4792475 -0.17058197 ## Bwt -0.1705820 0.06263081 summary(lm1)$sigma ## [1] 1.452373 3.7.3 Add ROxygen2 documentation Reading: http://kbroman.org/pkg_primer/pages/docs.html #&#39; Linear regression #&#39; #&#39; Runs an OLS regression not unlike \\code{\\link{lm}} #&#39; #&#39; @param y response vector (1 x n) #&#39; @param X covariate matrix (p x n) with no intercept #&#39; #&#39; @return A list with 4 elements: coefficients, vcov, sigma, df #&#39; #&#39; @examples #&#39; data(mtcars) #&#39; X &lt;- as.matrix(mtcars[, c(&quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;)]) #&#39; y &lt;- mtcars[, &quot;mpg&quot;] #&#39; linmodEst(y, X) #&#39; #&#39; @export #&#39; linmodEst &lt;- function(x, y) { ## CC: crossprod or a QR decomposition (as in the original version) are more efficient coef &lt;- solve(t(x) %*% x) %*% t(x) %*% y print(coef) ## degrees of freedom and standard deviation of residuals df &lt;- nrow(x) - ncol(x) sigma2 &lt;- sum((y - x %*% coef) ^ 2) / df ## compute sigma^2 * (x’x)^-1 vcov &lt;- sigma2 * solve(t(x) %*% x) colnames(vcov) &lt;- rownames(vcov) &lt;- colnames(x) list( coefficients = coef, vcov = vcov, sigma = sqrt(sigma2), df = df ) } 3.7.4 Configure Build Tools Figure 3.11: Roxygen options 3.7.5 man page File `man/linmodEst.Rd contains: % Generated by roxygen2: do not edit by hand % Please edit documentation in R/linreg.R \\name{linmodEst} \\alias{linmodEst} \\title{Linear regression} \\usage{ linmodEst(x, y) } \\arguments{ \\item{y}{response vector (1 x n)} \\item{X}{covariate matrix (p x n) with no intercept} } \\value{ A list with 4 elements: coefficients, vcov, sigma, df } \\description{ Runs an OLS regression not unlike \\code{\\link{lm}} } \\examples{ data(mtcars) X &lt;- as.matrix(mtcars[, c(&quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;)]) y &lt;- mtcars[, &quot;mpg&quot;] linmodEst(y, X) } 3.7.6 Formatted output 3.7.7 DESCRIPTION Reading: http://r-pkgs.had.co.nz/description.html Package: Linreg Type: Package Title: What the Package Does (Title Case) Version: 0.1.0 Author: Who wrote it Maintainer: The package maintainer &lt;yourself@somewhere.net&gt; Description: More about what it does (maybe more than one line) Use four spaces when indenting paragraphs within the Description. License: What license is it under? Encoding: UTF-8 LazyData: true RoxygenNote: 6.0.1 3.7.8 NAMESPACE Reading: http://r-pkgs.had.co.nz/namespace.html, in particular Imports vs Suggests export’s automatically generated when parsing ROxygen2 snippets export(linmodEst) A scary hack A scary tree Reading: https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging 3.7.9 S3 basics Reading: http://adv-r.had.co.nz/S3.html hello &lt;- function() { s &lt;- &quot;Hello World!&quot; class(s) &lt;- &quot;hi&quot; return(s) } hello() ## [1] &quot;Hello World!&quot; ## attr(,&quot;class&quot;) ## [1] &quot;hi&quot; print.hi &lt;- function(...) { print(&quot;Surprise!&quot;) } hello() ## [1] &quot;Surprise!&quot; 3.7.10 S3 and S4 generics Reading: http://adv-r.had.co.nz/S4.html linmod &lt;- function(x, ...) UseMethod(&quot;linmod&quot;) linmod.default &lt;- function(x, y, ...) { x &lt;- as.matrix(x) y &lt;- as.numeric(y) est &lt;- linmodEst(x, y) est$fitted.values &lt;- as.vector(x %*% est$coefficients) est$residuals &lt;- y - est$fitted.values est$call &lt;- match.call() class(est) &lt;- &quot;linmod&quot; return(est) } 3.7.11 print print.linmod &lt;- function(x, ...) { cat(&quot;Call:\\n&quot;) print(x$call) cat(&quot;\\nCoefficients:\\n&quot;) print(x$coefficients) } x &lt;- cbind(Const = 1, Bwt = cats$Bwt) y &lt;- cats$Hw mod1 &lt;- linmod(x, y) ## [,1] ## Const -0.3566624 ## Bwt 4.0340627 mod1 ## Call: ## linmod.default(x = x, y = y) ## ## Coefficients: ## [,1] ## Const -0.3566624 ## Bwt 4.0340627 3.7.12 Other methods summary.linmod print.summary.linmod predict.linmod plot.linmod coef.linmod, vcov.linmod, … Exercise 3.1 Write two functions that implement the coef.linmod and vcov.linmod methods. 3.7.13 Formulas and model frames Reading: http://genomicsclass.github.io/book/pages/expressing_design_formula.html model.frame (a generic function) and its methods return a data.frame with the variables needed to use formula and any … arguments. model.matrix creates a design (or model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly. model.response returns the response of a model frame passed as optional arguments to model.frame. Exercise 3.2 What is model.extract? linmod.formula &lt;- function(formula, data = list(), ...) { mf &lt;- model.frame(formula = formula, data = data) x &lt;- model.matrix(attr(mf, &quot;terms&quot;), data = mf) y &lt;- model.response(mf) est &lt;- linmod.default(x, y, ...) est$call &lt;- match.call() est$formula &lt;- formula return(est) } linmod(Hwt ~ - 1 + Bwt * Sex, data = cats) Call: linmod.formula(formula = Hwt ~ -1 + Bwt * Sex, data = cats) Coefficients: Bwt SexF SexM Bwt:SexM 2.636414 2.981312 -1.184088 1.676265 3.8 Unit testing 3.8.1 Unit tests and testthat Reading: http://r-pkgs.had.co.nz/tests.html In package directory: devtools::use_testthat() pre-populates test/testthat/ Test files should start with test to be processed. 3.8.2 test_coef.R data(cats, package = &quot;MASS&quot;) l1 &lt;- linmod(Hwt ~ Bwt * Sex, data = cats) l2 &lt;- lm(Hwt ~ Bwt * Sex, data = cats) test_that(&quot;same estimated coefficients as lm function&quot;, { expect_equal(round(l1$coefficients, 3), round(l2$coefficients, 3)) }) &gt; devtools::test() Loading Linreg Loading required package: testthat Testing Linreg . DONE ========================================================================================= 3.9 Continuous integration Readings: - http://r-pkgs.had.co.nz/check.html#travis - https://juliasilge.com/blog/beginners-guide-to-travis/ Website: https://travis-ci.org/ First step is to create a Travis account and link it to you GitHub account. Travis will list all your public GitHub repositories for you to select the ones you want to test. Calling devtools::use_coverage(pkg = &quot;.&quot;, type = c(&quot;codecov&quot;)) creates the .travis.yml file: # R for travis: see documentation at https://docs.travis-ci.com/user/languages/r language: R sudo: false cache: packages and pushing Linreg code to GitHub will automatically triggers a Travis build… which fails! To be continued… 3.10 Code coverage Reading: https://walczak.org/2017/06/how-to-add-code-coverage-codecov-to-your-r-package/ Website: https://codecov.io/ Like Travis, codecov has to be linked to a GitHub account: devtools::use_coverage(pkg = &quot;.&quot;, type = c(&quot;codecov&quot;)) creates the codecov.yml file: comment: false A call to covr::codecov(token = &quot;YOUR_TOKEN&quot;) will give you code coverage information: 3.11 Back to GitHub Badges can be added to README.md: &lt;!--- Badges -----&gt; [![Travis (LINUX) Build Status](https://travis-ci.org/cchoirat/Linreg.svg?branch=master)](https://travis-ci.org/cchoirat/Linreg) [![codecov](https://codecov.io/gh/cchoirat/Linreg/branch/master/graph/badge.svg)](https://codecov.io/gh/cchoirat/Linreg) ## `Linreg` package template Based on &quot;Creating R Packages: A Tutorial&quot; (Friedrich Leisch, 2009) - https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf are automatically displayed on GitHub: 3.12 Vignettes Reading: http://r-pkgs.had.co.nz/vignettes.html Reading: http://kbroman.org/pkg_primer/pages/vignettes.html Even if all the functions and datasets of your package are documented, it is still useful to have a more detailed illustation on how to use your package. A vignette is the right place to explain a worflow and a statistical method. Running: devtools::use_vignette(&quot;my-linear-regression&quot;) creates a vignettes folder and provide a template in RMarkdown format my-linear-regression.Rmd: https://github.com/cchoirat/Linreg/blob/master/vignettes/my-linear-regression.Rmd It also indicates in DESCRIPTION that vignettes should be built with knitr. VignetteBuilder: knitr The vignette is built into a HTML document with devtools::build_vignettes() Building Linreg vignettes Moving my-linear-regression.html, my-linear-regression.R to inst/doc/ Copying my-linear-regression.Rmd to inst/doc/ The vignette is accessible with vignette(&quot;my-linear-regression&quot;) vignette(&quot;my-linear-regression&quot;, package = &quot;Linreg&quot;) References "],
["optimization.html", "Chapter 4 Optimization 4.1 Measuring performance 4.2 Improving performance 4.3 Vectorization 4.4 Parallelization 4.5 Introduction to C++ 4.6 Rcpp packages 4.7 Getting serious about C++ 4.8 Profiling", " Chapter 4 Optimization In this Chapter, we will see how to measure and improve code performance. 4.1 Measuring performance 4.1.1 Benchmarking Reading: http://adv-r.had.co.nz/Performance.html#microbenchmarking There are several ways to benchmark code (see http://www.alexejgossmann.com/benchmarking_r/) from system.time to dedicated packages such as rbenchmark (Kusnierczyk (2012)) or microbenchmark (Mersmann (2015)). Let’s start with an example from Wickham (2014). library(microbenchmark) m &lt;- microbenchmark( times = 1000, # default is 100 &quot;[32, 11]&quot; = mtcars[32, 11], &quot;$carb[32]&quot; = mtcars$carb[32], &quot;[[c(11, 32)]]&quot; = mtcars[[c(11, 32)]], &quot;[[11]][32]&quot; = mtcars[[11]][32], &quot;.subset2&quot; = .subset2(mtcars, 11)[32] ) m ## Unit: nanoseconds ## expr min lq mean median uq max neval cld ## [32, 11] 8064 9821.0 11523.650 10500.0 11800.5 96373 1000 c ## $carb[32] 4368 5754.0 6775.816 6345.0 7128.0 54158 1000 b ## [[c(11, 32)]] 3713 4776.5 5658.275 5248.0 5915.5 53616 1000 b ## [[11]][32] 3280 4451.0 7652.222 4876.5 5528.5 2470991 1000 bc ## .subset2 193 272.0 362.460 314.0 380.0 3034 1000 a ggplot2::autoplot(m) 4.1.2 Profiling and optimization Reading: http://adv-r.had.co.nz/Profiling.html#measure-perf Let’s compare three ways of estimating a linear regression: with built-in lm and with two functions we defined in package Linreg in Chapter 3. library(Linreg) data(cats, package = &quot;MASS&quot;) fit1 &lt;- lm(Hwt ~ Bwt, data = cats) fit2 &lt;- linmod(Hwt ~ Bwt, data = cats) fit3 &lt;- linmodEst(cbind(1, cats$Bwt), cats$Hwt) ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 all.equal(round(coef(fit1), 5), round(coef(fit2), 5)) ## [1] &quot;names for target but not for current&quot; ## [2] &quot;Attributes: &lt; names for current but not for target &gt;&quot; ## [3] &quot;Attributes: &lt; Length mismatch: comparison on first 0 components &gt;&quot; ## [4] &quot;target is numeric, current is matrix&quot; all.equal(round(coef(fit1), 5), round(fit3$coefficients, 5), check.names = FALSE) ## [1] &quot;Attributes: &lt; names for current but not for target &gt;&quot; ## [2] &quot;Attributes: &lt; Length mismatch: comparison on first 0 components &gt;&quot; ## [3] &quot;target is numeric, current is matrix&quot; m &lt;- microbenchmark( fit1 &lt;- lm(Hwt ~ Bwt, data = cats), fit2 &lt;- linmod(Hwt ~ Bwt, data = cats), fit3 &lt;- linmodEst(cbind(1, cats$Bwt), cats$Hwt) # custom checks can be performed with the &#39;check&#39; argument ) ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 ## [,1] ## [1,] -0.3566624 ## [2,] 4.0340627 m ## Unit: microseconds ## expr min lq mean ## fit1 &lt;- lm(Hwt ~ Bwt, data = cats) 660.859 707.9735 944.8127 ## fit2 &lt;- linmod(Hwt ~ Bwt, data = cats) 514.046 567.9420 818.8203 ## fit3 &lt;- linmodEst(cbind(1, cats$Bwt), cats$Hwt) 95.260 123.2410 163.6884 ## median uq max neval cld ## 785.4320 950.9625 3434.681 100 b ## 624.2415 765.1990 8361.887 100 b ## 140.8255 173.2055 505.600 100 a ggplot2::autoplot(m) 4.2 Improving performance Vectorize Parallelize Use a faster language (C/C++, Fortran, …) Use different tools (as in Chapter 6) 4.3 Vectorization Let’s take an example from a blog post (that seems to be gone). It’s used in Wickham (2014, Section Case studies). vacc1a &lt;- function(age, female, ily) { p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily p &lt;- p * if (female) 1.25 else 0.75 p &lt;- max(0, p) p &lt;- min(1, p) p } set.seed(1959) n &lt;- 1000 age &lt;- rnorm(n, mean = 50, sd = 10) female &lt;- sample(c(T, F), n, rep = TRUE) ily &lt;- sample(c(T, F), n, prob = c(0.8, 0.2), rep = TRUE) vacc1a(age[1], female[1], ily[1]) ## [1] 0.1667005 vacc1a(age[2], female[2], ily[2]) ## [1] 0.4045439 vacc1a(age[3], female[3], ily[3]) ## [1] 0.2699324 vacc1a is not designed for vector inputs vacc1a(age, female, ily) ## Warning in if (female) 1.25 else 0.75: the condition has length &gt; 1 and ## only the first element will be used ## [1] 0.2526293 It should be called vacc1a(age[1], female[1], ily[1]) ## [1] 0.1667005 vacc1a(age[2], female[2], ily[2]) ## [1] 0.4045439 vacc1a(age[3], female[3], ily[3]) ## [1] 0.2699324 We can use a loop: out &lt;- numeric(n) for (i in 1:n) out[i] &lt;- vacc1a(age[i], female[i], ily[i]) or one of the apply functions: vacc0&lt;- function(age, female, ily) { sapply(1:n, function(i) vacc1a(age[i], female[i], ily[i])) } out0 &lt;- vacc0(age, female, ily) all.equal(out, out0) ## [1] TRUE But, it’s convenient for the function to support vector inputs, instead of relying on users writing their own wrappers. We can loop inside the function body. vacc1 &lt;- function(age, female, ily) { n &lt;- length(age) out &lt;- numeric(n) for (i in seq_len(n)) { out[i] &lt;- vacc1a(age[i], female[i], ily[i]) } out } or we can rely on base R functions that accept vector inputs vacc2 &lt;- function(age, female, ily) { p &lt;- 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily p &lt;- p * ifelse(female, 1.25, 0.75) p &lt;- pmax(0, p) p &lt;- pmin(1, p) p } 4.4 Parallelization library(parallel) cores &lt;- detectCores() cores ## [1] 8 vacc3 &lt;- function(age, female, ily) { mcmapply(function(i) vacc1a(age[i], female[i], ily[i]), 1:n, mc.cores = cores - 1) } out3 &lt;- vacc3(age, female, ily) library(microbenchmark) m &lt;- microbenchmark( vacc0 = vacc0(age, female, ily), vacc1 = vacc1(age, female, ily), vacc2 = vacc2(age, female, ily), vacc3 = vacc3(age, female, ily) ) m ## Unit: microseconds ## expr min lq mean median uq max neval ## vacc0 1894.204 3398.816 4418.2977 3882.422 5096.124 11246.478 100 ## vacc1 2154.973 2478.707 3439.0037 2941.113 3860.746 18848.996 100 ## vacc2 91.445 173.395 745.3954 267.081 1035.971 6067.985 100 ## vacc3 22240.615 24451.540 28646.5685 27403.684 31510.960 57419.594 100 ## cld ## b ## b ## a ## c ggplot2::autoplot(m) So, what’s going on? We will talk more about parallelization tools and techniques in Chapter `(???)(bigdata). 4.5 Introduction to C++ C++ is a very powerful object-oriented language. Many tutorials are available on-line, for example http://www.cplusplus.com/doc/tutorial/. R is intepreted, C++ is compiled and typically much faster (in loops for examples). Our introduction to C++ is from an R perspective. Python (and most interpreted languages) can be extended with C++ too. 4.5.1 Rcpp Reading: http://adv-r.had.co.nz/Rcpp.html Rcpp Eddelbuettel (2013) makes it very easy to use C++ code in R (for example to speed up a function or to wrap methods already implemented in C++). Rcpp provides “syntactic sugar” that makes is easy to leverage C++ even without a deep knowledge of it. To use Rcpp, you need a C++ compiler: Windows: Rtools OS X: Xcode Linux: r-base-dev from package manager 4.5.2 Hello World! library(Rcpp) cppFunction(&#39;void hello(){ Rprintf(&quot;Hello, world!&quot;); }&#39;) hello ## function () ## invisible(.Primitive(&quot;.Call&quot;)(&lt;pointer: 0x10e1c4de0&gt;)) hello() ## Hello, world! Rprintf is the counterpart of C++ printf function. Let’s take the first example of Wickham (2014), Section Getting started with C++. cppFunction(&#39;int add(int x, int y, int z) { int sum = x + y + z; return sum; }&#39;) We have to specify the input type and the output type. As expected add(1, 2, 3) returns 6. How about? add(1.1, 2.2, 3.3) cppFunction(&#39;double addd(double x, double y, double z) { double sum = x + y + z; return sum; }&#39;) With addd we do get 6.6: addd(1.1, 2.2, 3.3) 4.5.3 sourceCpp When C++ code takes more than a couple of lines, it’s more convenient to create a stand-alone C++ source file. From the RStudio default template: #include &lt;Rcpp.h&gt; using namespace Rcpp; NumericVector timesTwo(NumericVector x) { return x * 2; } /*** R timesTwo(42) */ From R, we can use sourceCpp to access timesTwo in R: sourceCpp(&quot;src/times-two.cpp&quot;) timesTwo(100) 4.5.4 Data types int double bool string NumericVector LogicalVector IntegerVector CharacterVector NumericMatrix IntegerMatrix LogicalMatrix CharacterMatrix NA_REAL NA_INTEGER NA_STRING NA_LOGICAL List DataFrame Function … 4.5.5 Sugar Reading: http://adv-r.had.co.nz/Rcpp.html#rcpp-sugar. Vectorization of +, *, -, /, pow, &lt;, &lt;=, &gt;, &gt;=, ==, !=, … Vectorization of R-like functions: abs(), exp(), factorial(), … Exercise 4.1 Can you write an Rcpp function similar to addd but accepting vector arguments? cppFunction(&#39;NumericVector addv(NumericVector x, NumericVector y, NumericVector z) { NumericVector sum = x + y + z; return sum; }&#39;) 4.5.6 Example (continued) #include &lt;Rcpp.h&gt; using namespace Rcpp; double vacc3a(double age, bool female, bool ily){ double p = 0.25 + 0.3 * 1 / (1 - exp(0.04 * age)) + 0.1 * ily; p = p * (female ? 1.25 : 0.75); p = std::max(p, 0.0); p = std::min(p, 1.0); return p; } // [[Rcpp::export]] NumericVector vacc3(NumericVector age, LogicalVector female, LogicalVector ily) { int n = age.size(); NumericVector out(n); for(int i = 0; i &lt; n; ++i) { out[i] = vacc3a(age[i], female[i], ily[i]); } return out; } 4.5.7 Back to Linreg armadillo is a very powerful C++ linear algebra library: http://arma.sourceforge.net/ It can be used in Rcpp via the RcppArmadillo package. Exercise 4.2 Can you write an Rcpp function similar to linmodEst? linmodEst &lt;- function(x, y) { ## CC: crossprod or a QR decomposition (as in the original version) are more efficient coef &lt;- solve(t(x) %*% x) %*% t(x) %*% y ## degrees of freedom and standard deviation of residuals df &lt;- nrow(x) - ncol(x) sigma2 &lt;- sum((y - x %*% coef) ^ 2) / df ## compute sigma^2 * (x’x)^-1 vcov &lt;- sigma2 * solve(t(x) %*% x) colnames(vcov) &lt;- rownames(vcov) &lt;- colnames(x) list( coefficients = coef, vcov = vcov, sigma = sqrt(sigma2), df = df ) } 4.6 Rcpp packages Readings: - https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-package.pdf - http://adv-r.had.co.nz/Rcpp.html#rcpp-package 4.7 Getting serious about C++ 4.7.1 STL STL: Standard Template Library Reading: http://adv-r.had.co.nz/Rcpp.html#stl 4.8 Profiling Reading: https://rstudio.github.io/profvis/ library(profvis) profvis({ data(diamonds, package = &quot;ggplot2&quot;) plot(price ~ carat, data = diamonds) m &lt;- lm(price ~ carat, data = diamonds) abline(m, col = &quot;red&quot;) }) References "],
["databases.html", "Chapter 5 Databases 5.1 What is SQL? 5.2 SQLite: An Exercise 5.3 SQL and R 5.4 Non-Relational Databases (noSQL) 5.5 References 5.6 NoSQL: MongoDB", " Chapter 5 Databases 5.1 What is SQL? SQL (Structured Query Language) is a standard way of specifying the information you want to receive from a database. There are a number of variations on the language, and a number of online resources available for learning their various complexities. However, the general structure of all SQL queries is consistent across implementations. SQL is an imperative computer language. This means that it describes the output desired without actually describing the calculations required to get the output described. This allows for the verbs and structures of the language to be used across database systems, as well as in other areas of data handling. 5.1.1 What is a database? A database is simply an organized structure for storing and accessing data on disk. There are a number of structures used to store data on disk, each with their own languages. However, despite the variations in structure, the goals (and song) remain the same. The process of data storage on disk is controlled by the database management system (DBMS). 5.1.2 Relational Databases (SQL) The most common type of DBMS is a relational database (RDBMS). A Relational Database stores information in a the form of entities and the relationships between them. Entities are typically nouns and relationships are typically verbs. For example, if we wanted to store information about class enrollment at a university, the entities would consist of objects like a student, class, and professor. The relationships would consist of takes and teaches. Relationships can be one to one, many to many, or one to many. 5.1.3 Types of Relational Databases Commercial Oracle Database Microsoft SQL Server … Open-source MySQL PostgreSQL SQLite … SQLite is the easiest way to start: unlike the others, it’s not a client-server DB. The whole DB can live in a (portable) folder. All the required tools are included in dplyr. 5.1.4 SQL In relational databases, entities and relationships are represented by tables, where each row or record in a table represents a particular instance of of that general object. Continuing the class example, students would be stored in Student, classes in Class, and professors in Professor. The table containing the relationships between students and classes would be likely named StudentClass and the The three key parts of a SQL query are the SELECT clause, the FROM clause, and the WHERE clause. The SELECT clause specifies the pieces of information you want about an individual record, the FROM clause specifies the tables that will be used To get all information about all students we would type the following: SELECT * FROM STUDENT To Select the name and birthday of all students in classes taught by Dr. Choirat would be a more complex query, which would likely look something like this: SELECT Name, DOB FROM Student s inner join StudentClass sc on s.ID = sc.studentid inner join ProfessorClass pc on sc.classid = pc.classid inner join Professor p on pc.profid = p.id WHERE p.lastname = &quot;Choirat&quot; 5.2 SQLite: An Exercise Create an in memory DB sqlite3 5.2.1 Make a Table CREATE TABLE table1(x,y,z); 5.2.2 Insert Values INSERT INTO table1 VALUES (1,2,3),(4,5,6),(7,8,9); 5.2.3 Select Values Select All Values SELECT * FROM table1; Select specific values SELECT z from table1 WHERE x = 4; 5.3 SQL and R There are a number of R packages for interfacing directly with RDBs. RODBC is one sucn example that allows for queries to be submitted to previsously set up database connections with the results being returned as a data frame for further analysis in R. There’s a large amount of documentation available online for these methods. Each system has its own idiosyncracies. 5.3.1 Data: oscars and movies again: 2016 Oscars Nominations library(readr) library(dplyr) db &lt;- src_sqlite(&quot;db.sqlite3&quot;, create = TRUE) oscars &lt;-&quot; name,movie,category Adam McKay,The Big Short,Best Director Alejandro González Iñárritu,The Revenant,Best Director Lenny Abrahamson,Room,Best Director Tom McCarthy,Spotlight,Best Director George Miller,Mad Max: Fury Road,Best Director Bryan Cranston,Trumbo,Best Actor Matt Damon,The Martian,Best Actor Michael Fassbender,Steve Jobs,Best Actor Leonardo DiCaprio,The Revenant,Best Actor Eddie Redmayne,The Danish Girl,Best Actor Cate Blanchett,Carol,Best Actress Brie Larson,Room,Best Actress Jennifer Lawrence,Joy,Best Actress Charlotte Rampling,45 Years,Best Actress Saoirse Ronan,Brooklyn,Best Actress &quot; oscars &lt;- read_csv(oscars, trim_ws = TRUE, skip = 1) movies &lt;-&quot; movie,length_mins The Big Short,130 Star Wars: The Force Awakens,135 Brooklyn,111 Mad Max: Fury Road,120 Room,118 The Martian,144 The Revenant,156 Spotlight,128 &quot; movies &lt;- read_csv(movies, trim_ws = TRUE, skip = 1) oscars_table &lt;- copy_to(db, oscars) movies_table &lt;- copy_to(db, movies) db 5.4 Non-Relational Databases (noSQL) 5.4.1 Drawbacks of Relational Databases Looking up all information about one entity can be expensive Require a large amount of overhead Difficult to distribute across multiple disks Considered to by some to be inflexible 5.4.2 Common Types of NoSQL Databases Graph Databases Neo4j OrientDB Document Databases MongoDB JSON Databases XML Databases 5.5 References The Oscar movie example comes from this lecture by Rafa Irizarry: https://github.com/datasciencelabs/2016/blob/master/lectures/wrangling/data-wrangling-with-dplyr.Rmd 5.6 NoSQL: MongoDB 5.6.1 JSON format JSON: JavaScript Object Notation. Readings: http://www.json.org/ http://json.org/example.html https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html library(jsonlite) l &lt;- fromJSON( &#39;{ &quot;glossary&quot;: { &quot;title&quot;: &quot;example glossary&quot;, &quot;GlossDiv&quot;: { &quot;title&quot;: &quot;S&quot;, &quot;GlossList&quot;: { &quot;GlossEntry&quot;: { &quot;ID&quot;: &quot;SGML&quot;, &quot;SortAs&quot;: &quot;SGML&quot;, &quot;GlossTerm&quot;: &quot;Standard Generalized Markup Language&quot;, &quot;Acronym&quot;: &quot;SGML&quot;, &quot;Abbrev&quot;: &quot;ISO 8879:1986&quot;, &quot;GlossDef&quot;: { &quot;para&quot;: &quot;A meta-markup language, used to create markup languages such as DocBook.&quot;, &quot;GlossSeeAlso&quot;: [&quot;GML&quot;, &quot;XML&quot;] }, &quot;GlossSee&quot;: &quot;markup&quot; } } } } }&#39; ) l$glossary$title ## [1] &quot;example glossary&quot; l$glossary$GlossDiv$GlossList$GlossEntry$GlossDef ## $para ## [1] &quot;A meta-markup language, used to create markup languages such as DocBook.&quot; ## ## $GlossSeeAlso ## [1] &quot;GML&quot; &quot;XML&quot; l &lt;- fromJSON(&quot;src/example.json&quot;) 5.6.2 Reading a JSON file l &lt;- fromJSON(&quot;~/Dropbox/Data17/citibike/stations_2017-11-25.json&quot;&quot;) 5.6.3 RESTful APIs REST: Representational State Transfer Readings: https://spring.io/understanding/REST https://en.wikipedia.org/wiki/Representational_state_transfer#Applied_to_Web_services https://cran.r-project.org/web/packages/jsonlite/vignettes/json-apis.html 5.6.4 Back and forth l &lt;- list( a = data.frame(v1 = 1:5, v2 = letters[1:5]), b = list(el1 = 4, el2 = &quot;hello&quot;) ) l ## $a ## v1 v2 ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e ## ## $b ## $b$el1 ## [1] 4 ## ## $b$el2 ## [1] &quot;hello&quot; toJSON(l) ## {&quot;a&quot;:[{&quot;v1&quot;:1,&quot;v2&quot;:&quot;a&quot;},{&quot;v1&quot;:2,&quot;v2&quot;:&quot;b&quot;},{&quot;v1&quot;:3,&quot;v2&quot;:&quot;c&quot;},{&quot;v1&quot;:4,&quot;v2&quot;:&quot;d&quot;},{&quot;v1&quot;:5,&quot;v2&quot;:&quot;e&quot;}],&quot;b&quot;:{&quot;el1&quot;:[4],&quot;el2&quot;:[&quot;hello&quot;]}} toJSON(l, pretty = TRUE) ## { ## &quot;a&quot;: [ ## { ## &quot;v1&quot;: 1, ## &quot;v2&quot;: &quot;a&quot; ## }, ## { ## &quot;v1&quot;: 2, ## &quot;v2&quot;: &quot;b&quot; ## }, ## { ## &quot;v1&quot;: 3, ## &quot;v2&quot;: &quot;c&quot; ## }, ## { ## &quot;v1&quot;: 4, ## &quot;v2&quot;: &quot;d&quot; ## }, ## { ## &quot;v1&quot;: 5, ## &quot;v2&quot;: &quot;e&quot; ## } ## ], ## &quot;b&quot;: { ## &quot;el1&quot;: [4], ## &quot;el2&quot;: [&quot;hello&quot;] ## } ## } ll &lt;- fromJSON(toJSON(l)) 5.6.5 MongoDB Reading: https://docs.mongodb.com/manual/administration/install-community/ With homebrew on OS X: brew update brew install mongodb brew tap homebrew/services # once brew services start mongodb 5.6.6 Querying data Reading: https://jeroen.github.io/mongolite/query-data.html 5.6.7 Example: mHealth data system(&quot;mongoimport --db mhealth --collection sleep --drop --file ~/Dropbox/Data17/mHealth/sleep-duration.json&quot;) library(mongolite) mhealth &lt;- mongo(db = &quot;mhealth&quot;) sleep &lt;- mongo(collection = &quot;sleep&quot;, db = &quot;mhealth&quot;) sleep$count(&#39;{}&#39;) # 52 records alldata &lt;- sleep$find(&#39;{}&#39;) alldata sleep$find() "],
["bigdata.html", "Chapter 6 Big data 6.1 How to deal with (very / too) large datasets? 6.2 How big is big? 6.3 List of tools 6.4 Data that fits in memory 6.5 Data that doesn’t fit in memory (but fits on drive) 6.6 Pure R solutions 6.7 Scaling up 6.8 Parallel computing and clusters 6.9 Cloud computing 6.10 h2o: “Fast Scalable Machine Learning” 6.11 Running h20 locally within R 6.12 JVM (from Wikipedia) 6.13 Which languages? (from Wikipedia) 6.14 Which languages? 6.15 State of the h2o JVM 6.16 Spark 6.17 Sparkling Water 6.18 Adding new models to h2o and spark 6.19 More? 6.20 Amazon Web Services (AWS) 6.21 Spark on AWS: Amazon Elastic MapReduce (EMR)", " Chapter 6 Big data 6.1 How to deal with (very / too) large datasets? Use more RAM / processors / drive space… Use less data: (re)sample, … Use a database Use specific R packages (ff, bigmemory) Use other tools 6.2 How big is big? Fits in RAM and on drive (but slow) Doesn’t fit in RAM but fits on drive Doesn’t fit in RAM and doesn’t fit on drive 6.3 List of tools Reading: Varian (2014) (PDF available) Spark? h2o? More? Let’s go back to the bottlenecks CPU RAM I/O 6.4 Data that fits in memory 6.4.1 Faster I/O Reading: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html data.table provides an enhanced of a data.frame and faster I/O with fread and fwrite. To read the 0.5GB ratings file from MovieLens library(data.table) system.time(ratings &lt;- fread(&quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;)) takes Read 20000263 rows and 4 (of 4) columns from 0.497 GB file in 00:00:05 user system elapsed 4.007 0.229 4.244 while system.time(ratings &lt;- read.csv(&quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;)) takes user system elapsed 85.199 2.711 90.997 There are ways to improve the speed of read.csv (for example, but specifying column types). But in general fread is much faster. library(readr) # in tidyverse system.time(ratings &lt;- read_csv(&quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;)) user system elapsed 10.290 3.037 18.450 also tends to perform better than read.csv. Table 6.1: I/O comparison package function. speed output base read.csv slow data.frame data.table fread very fast data.table readr read_csv fast tibble 6.4.2 Reference vs copy Reading: http://adv-r.had.co.nz/memory.html Reading: https://jangorecki.gitlab.io/data.table/library/data.table/html/assign.html library(pryr) library(data.table) d &lt;- read.csv(&quot;~/Dropbox/Data17/ml-latest-small/ratings.csv&quot;) D &lt;- fread(&quot;~/Dropbox/Data17/ml-latest-small/ratings.csv&quot;) object_size(d) object_size(D) mem_change(d$Idx &lt;- 1:nrow(d)) mem_change(D[, Idx:= 1:.N]) object_size(d$Idx) object_size(D$Idx) d &lt;- read.csv(&quot;~/Dropbox/Data17/ml-latest-small/ratings.csv&quot;) D &lt;- fread(&quot;~/Dropbox/Data17/ml-latest-small/ratings.csv&quot;) .Internal(inspect(d)) d$Idx &lt;- 1:nrow(d) .Internal(inspect(d)) .Internal(inspect(D)) D[, Idx:= 1:.N] .Internal(inspect(D)) 6.4.3 data.table: another data manipulation grammar Reading: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html Exercise 6.1 Benchmark adding a column to a large data frame vs a large data table. 6.5 Data that doesn’t fit in memory (but fits on drive) Let’s try to work with a 12GB file and 4/8 GB of memory… 6.6 Pure R solutions 6.6.1 A regressions example library(data.table) airlines &lt;- fread(&quot;/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.csv&quot;) rfit &lt;- lm(ArrDelay ~ Distance, data = airlines) summary(rfit) 6.6.2 Sampling Read the data (even line by line) Select a sample of rows Run your model on the random sample 6.6.3 bigmemory https://cran.r-project.org/web/packages/bigmemory/index.html Reading: https://cran.r-project.org/web/packages/bigmemory/vignettes/Overview.pdf bigmemory: Manage Massive Matrices with Shared Memory and Memory-Mapped Files Create, store, access, and manipulate massive matrices. Matrices are allocated to shared memory and may use memory-mapped files. Packages ‘biganalytics’, ‘bigtabulate’, ‘synchronicity’, and ‘bigalgebra’ provide advanced functionality. (+) pure R solution from a user perspective (-) mostly for numeric data matrices, mostly to speed up computations on data of +/- RAM size library(bigmemory) library(biganalytics) # library(bigtabulate) # library(biglm) flights &lt;- read.big.matrix( &quot;/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.csv&quot;, header = TRUE, backingfile = &quot;allyears2k.bin&quot;, backingpath = &quot;/Users/cchoirat/Dropbox/Data17/AirFlights/&quot;, descriptorfile = &quot;allyears2k.desc&quot;, shared = TRUE) air_flights &lt;- attach.big.matrix(&quot;/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.desc&quot;) dim(air_flights) colnames(air_flights) mean(air_flights[, &quot;ArrDelay&quot;], na.rm = TRUE) fit &lt;- biglm.big.matrix(ArrDelay ~ Distance, data = air_flights) fit summary(fit) 6.6.4 Database connections and lazy evaluation library(data.table) D &lt;- fread(&quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;) library(sqldf) read.csv.sql(file = &quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;, sql = c(&quot;ATTACH &#39;ratings.sqlite3&#39; AS NEW&quot;)) read.csv.sql(file = &quot;~/Dropbox/Data17/ml-20m/ratings.csv&quot;, sql = &quot;CREATE TABLE ratings_table AS SELECT * FROM file&quot;, dbname = &quot;ratings.sqlite3&quot;) library(dplyr) library(DBI) con &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = &quot;ratings.sqlite3&quot;) ratings_db &lt;- tbl(con, &quot;ratings_table&quot;) ratings_db %&gt;% select(ends_with(&quot;Id&quot;)) %&gt;% filter(movieId &lt; 100) # Source: lazy query [?? x 2] # Database: sqlite 3.19.3 # [/Users/cchoirat/Documents/LocalGit/bigdata17/ratings.sqlite3] userId movieId &lt;int&gt; &lt;int&gt; 1 1 2 2 1 29 3 1 32 4 1 47 5 1 50 6 2 3 7 2 62 8 2 70 9 3 1 10 3 24 # ... with more rows # ... with more rows ratings_db %&gt;% select(ends_with(&quot;Id&quot;)) %&gt;% filter(movieId &lt; 100) %&gt;% collect() # A tibble: 790,226 x 2 userId movieId &lt;int&gt; &lt;int&gt; 1 1 2 2 1 29 3 1 32 4 1 47 5 1 50 6 2 3 7 2 62 8 2 70 9 3 1 10 3 24 # ... with 790,216 more rows 6.7 Scaling up 6.8 Parallel computing and clusters 6.9 Cloud computing More soon with the Odyssey guest lecture (https://www.rc.fas.harvard.edu/odyssey/). 6.10 h2o: “Fast Scalable Machine Learning” http://www.h2o.ai/ http://www.r-bloggers.com/scalable-machine-learning-for-big-data-using-r-and-h2o/ http://venturebeat.com/2014/11/07/h2o-funding/ https://www.h2o.ai/driverless-ai/ https://www.infoworld.com/article/3236048/machine-learning/review-h2oai-automates-machine-learning.html 6.10.1 Ecosystem Readings: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html http://www.h2o.ai/download/h2o/r https://cran.r-project.org/web/packages/h2o/index.html To build H2O or run H2O tests, the 64-bit JDK is required. To run the H2O binary using either the command line, R, or Python packages, only 64-bit JRE is required. if (&quot;package:h2o&quot; %in% search()) { detach(&quot;package:h2o&quot;, unload=TRUE) } if (&quot;h2o&quot; %in% rownames(installed.packages())) { remove.packages(&quot;h2o&quot;) } install.packages(&quot;h2o&quot;) 6.11 Running h20 locally within R library(h2o) localH2O &lt;- h2o.init(min_mem_size = &quot;32g&quot;) # h2o.init(ip = &quot;localhost&quot;, port = 54321, startH2O = TRUE, # forceDL = FALSE, enable_assertions = TRUE, license = NULL, # nthreads = -2, max_mem_size = NULL, min_mem_size = NULL, # ice_root = tempdir(), strict_version_check = TRUE, # proxy = NA_character_, https = FALSE, insecure = FALSE, # username = NA_character_, password = NA_character_) Connection successful! R is connected to the H2O cluster: H2O cluster uptime: 19 days 12 hours H2O cluster version: 3.14.0.3 H2O cluster version age: 1 month and 24 days H2O cluster name: H2O_started_from_R_cchoirat_bgt310 H2O cluster total nodes: 1 H2O cluster total memory: 30.67 GB H2O cluster total cores: 8 H2O cluster allowed cores: 8 H2O cluster healthy: TRUE H2O Connection ip: localhost H2O Connection port: 54321 H2O Connection proxy: NA H2O Internal Security: FALSE H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 R Version: R version 3.4.2 (2017-09-28) 6.12 JVM (from Wikipedia) A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program. There are three notions of the JVM: specification, implementation, and instance. The specification is a document that formally describes what is required of a JVM implementation. (https://en.wikipedia.org/wiki/Java_virtual_machine) 6.13 Which languages? (from Wikipedia) This list of JVM Languages comprises notable computer programming languages that are used to produce software that runs on the Java Virtual Machine (JVM). Some of these languages are interpreted by a Java program, and some are compiled to Java bytecode and JIT-compiled during execution as regular Java programs to improve performance. 6.14 Which languages? https://en.wikipedia.org/wiki/List_of_JVM_languages Java Scala, an object-oriented and functional programming language Jython R (an implementation of R: https://en.wikipedia.org/wiki/Renjin) … 6.15 State of the h2o JVM h2o.clusterInfo() R is connected to the H2O cluster: H2O cluster uptime: 19 days 13 hours H2O cluster version: 3.14.0.3 H2O cluster version age: 1 month and 24 days H2O cluster name: H2O_started_from_R_cchoirat_bgt310 H2O cluster total nodes: 1 H2O cluster total memory: 30.67 GB H2O cluster total cores: 8 H2O cluster allowed cores: 8 H2O cluster healthy: TRUE H2O Connection ip: localhost H2O Connection port: 54321 H2O Connection proxy: NA H2O Internal Security: FALSE H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 R Version: R version 3.4.2 (2017-09-28) Let’s check http://localhost:54321/flow/index.html. 6.15.1 Importing data into h2o from the R session data(cars) cars_to_h2o &lt;- as.h2o(cars, destination_frame = &quot;cars_from_r&quot;) is.data.frame(cars_to_h2o) # FALSE class(cars_to_h2o) # H2OFrame (No persistence beyond the R session when h2O is started from R.) 6.15.2 h2o functions summary(cars_to_h2o) # actually calls h2o:::summary.H2OFrame(cars_to_h2o) Approximated quantiles computed! If you are interested in exact quantiles, please pass the `exact_quantiles=TRUE` parameter. speed dist Min. : 4.0 Min. : 2.00 1st Qu.:12.0 1st Qu.: 26.00 Median :15.0 Median : 36.00 Mean :15.4 Mean : 42.98 3rd Qu.:19.0 3rd Qu.: 56.00 Max. :25.0 Max. :120.00 6.15.3 Let’s check from the h2o JVM From the browser: ‘Data’ -&gt; ‘List All Frames’ 6.15.4 Importing data into h2o from disk airlines_path &lt;- &quot;/Users/cchoirat/Dropbox/Data17/AirFlights/allyears2k.csv&quot; # full path airlines_to_h2o &lt;- h2o.importFile(path = airlines_path, destination_frame = &quot;airlines_from_r&quot;) summary(airlines_to_h2o) 6.15.5 Running a statistical model h2ofit &lt;- h2o.glm(y = &quot;ArrDelay&quot;, x = &quot;Distance&quot;, training_frame = airlines_to_h2o, intercept = TRUE, # default is TRUE family = &quot;gaussian&quot;) h2ofit summary(h2ofit) Coefficients: glm coefficients names coefficients standardized_coefficients 1 Intercept 7.702657 9.308665 2 Distance 0.002199 1.272253 airlines &lt;- read.csv(&quot;~/Dropbox/Data17/AirFlights/allyears2k.csv&quot;) rfit &lt;- lm(ArrDelay ~ Distance, data = airlines) summary(rfit) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.7011553 0.2326198 33.106 &lt;2e-16 *** Distance 0.0022045 0.0002487 8.865 &lt;2e-16 *** 6.15.6 h2o models 6.15.7 Closing the h2o session h2o.shutdown() 6.15.8 Available algorithms Deep learning Distributed randon forest Gradient boosting method Generalized linear modeling Generalized low rank modeling K-means Naive Bayes Principal component analysis 6.15.9 How to incorporate new models in h2o? (+) built-in models behave very much like R and are scalable (-) not easy to extend (e.g., GLM https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/main/java/hex/glm/GLM.java) Exercise 6.2 Run the same analysis with the whole dataset allyears.csv. 6.16 Spark Reading: https://spark.rstudio.com/ library(sparklyr) spark_install(version = &quot;2.1.0&quot;) conf &lt;- spark_config() conf$`sparklyr.shell.driver-memory` &lt;- &quot;32G&quot; conf$spark.memory.fraction &lt;- 0.5 sc &lt;- spark_connect(master = &quot;local&quot;) library(dplyr) iris_tbl &lt;- copy_to(sc, iris) flights_tbl &lt;- copy_to(sc, nycflights13::flights, &quot;flights&quot;) batting_tbl &lt;- copy_to(sc, Lahman::Batting, &quot;batting&quot;) src_tbls(sc) [1] &quot;batting&quot; &quot;flights&quot; &quot;iris&quot; You can use SQL: library(DBI) iris_preview &lt;- dbGetQuery(sc, &quot;SELECT * FROM iris LIMIT 10&quot;) Like h2o, you can open a web interface: spark_web(sc) top_rows &lt;- read.csv(&quot;~/Dropbox/Data17/AirFlights/allyears.csv&quot;, nrows = 5) file_columns &lt;- top_rows %&gt;% purrr::map(function(x)&quot;character&quot;) rm(top_rows) sp_flights &lt;- spark_read_csv(sc, name = &quot;flights2&quot;, path = &quot;~/Dropbox/Data17/AirFlights/allyears.csv&quot;, memory = FALSE, columns = file_columns, infer_schema = FALSE) # Source: table&lt;flights2&gt; [?? x 31] # Database: spark_connection Year Month DayofMonth DayOfWeek DepTime CRSDepTime ArrTime CRSArrTime UniqueCarrier &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1987 10 14 3 741 730 912 849 PS 2 1987 10 15 4 729 730 903 849 PS 3 1987 10 17 6 741 730 918 849 PS 4 1987 10 18 7 729 730 847 849 PS 5 1987 10 19 1 749 730 922 849 PS 6 1987 10 21 3 728 730 848 849 PS 7 1987 10 22 4 728 730 852 849 PS 8 1987 10 23 5 731 730 902 849 PS 9 1987 10 24 6 744 730 908 849 PS 10 1987 10 25 7 729 730 851 849 PS # ... with more rows, and 22 more variables: FlightNum &lt;chr&gt;, TailNum &lt;chr&gt;, # ActualElapsedTime &lt;chr&gt;, CRSElapsedTime &lt;chr&gt;, AirTime &lt;chr&gt;, ArrDelay &lt;chr&gt;, # DepDelay &lt;chr&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;chr&gt;, TaxiIn &lt;chr&gt;, # TaxiOut &lt;chr&gt;, Cancelled &lt;chr&gt;, CancellationCode &lt;chr&gt;, Diverted &lt;chr&gt;, # CarrierDelay &lt;chr&gt;, WeatherDelay &lt;chr&gt;, NASDelay &lt;chr&gt;, SecurityDelay &lt;chr&gt;, # LateAircraftDelay &lt;chr&gt;, IsArrDelayed &lt;chr&gt;, IsDepDelayed &lt;chr&gt; flights_table &lt;- sp_flights %&gt;% mutate(DepDelay = as.numeric(DepDelay), ArrDelay = as.numeric(ArrDelay), Distance = as.numeric(Distance), SchedDeparture = as.numeric(CRSDepTime)) %&gt;% select(Origin, Dest, SchedDeparture, ArrDelay, DepDelay, Month, DayofMonth, Distance) flights_table %&gt;% head Cache data: sp_flights %&gt;% tally # takes a looooong time 123534969… # might take a while... subset_table &lt;- flights_table %&gt;% compute(&quot;flights_subset&quot;) subset_table %&gt;% tally # a bit faster. 123534969 as well! 6.16.1 Run a statistical model # small_flights &lt;- spark_read_csv(sc, # name = &quot;flights2&quot;, # path = &quot;~/Dropbox/Data17/AirFlights/allyears2k.csv&quot;, # memory = FALSE, # columns = file_columns, # infer_schema = FALSE) # small_flights_table &lt;- small_flights %&gt;% # mutate(DepDelay = as.numeric(DepDelay), # ArrDelay = as.numeric(ArrDelay), # Distance = as.numeric(Distance), # SchedDeparture = as.numeric(CRSDepTime)) %&gt;% # select(Origin, Dest, SchedDeparture, ArrDelay, DepDelay, Month, DayofMonth, Distance) # # small_flights_table %&gt;% head # lm(arr_delay ~ distance, data = flights_tbl) ml_linear_regression(flights_table, response = &quot;ArrDelay&quot;, features = &quot;Distance&quot;) Coefficients: (Intercept) Distance 6.9707048955 0.0001100521 6.16.2 Deployment Reading: https://spark.rstudio.com/deployment.html 6.17 Sparkling Water Reading: https://spark.rstudio.com/h2o.html 6.18 Adding new models to h2o and spark 6.19 More? Reading: http://www.parallelr.com/r-gpu-programming-for-all-with-gpur/ 6.20 Amazon Web Services (AWS) AWS Management Console: https://aws.amazon.com/console/ Amazon EC2 Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Let’s launch an Amazon Linux instance (t2.micro free tier eligible) and configure a key pair. File permissions: https://www.tutorialspoint.com/unix/unix-file-permission.htm Key pairs: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html To connect: chmod 400 ~/Downloads/bst262-demo.pem ssh -i ~/Downloads/bst262-demo.pem ec2-user@ec2-18-217-97-206.us-east-2.compute.amazonaws.com __| __|_ ) _| ( / Amazon Linux AMI ___|\\___|___| We install R sudo yum update sudo yum list R-\\* sudo yum install R and Rstudio server (# https://www.rstudio.com/products/rstudio/download-server/) wget https://download2.rstudio.org/rstudio-server-rhel-1.1.383-x86_64.rpm sudo yum install --nogpgcheck rstudio-server-rhel-1.1.383-x86_64.rpm We need to add user and password: sudo useradd rserver sudo passwd rserver RStudio server uses port 8787. Let’s fix the security groups: We can now connect to RStudio server: http://ec2-18-217-97-206.us-east-2.compute.amazonaws.com:8787/ 6.21 Spark on AWS: Amazon Elastic MapReduce (EMR) https://aws.amazon.com/emr/ https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/ 6.21.1 Amazon S3 GUI or CLI 6.21.2 Amazon CLI https://aws.amazon.com/cli/ References "],
["visualization.html", "Chapter 7 Visualization 7.1 Maps and GIS 7.2 Principles of visualization", " Chapter 7 Visualization 7.1 Maps and GIS GIS: Geographic Information System Readings: https://bookdown.org/robinlovelace/geocompr/intro.html Bivand, Pebesma, and Gómez-Rubio (2013) http://rspatial.org/index.html http://www.spatialecology.com/beyer/assets/Beyer_Introduction_to_geospatial_analysis_in_R.pdf 7.1.1 Longitudes, latitudes and CRS Reading: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf CRS: Coordinate Reference System EPSG Geodetic Parameter Dataset: structured dataset of Coordinate Reference Systems and Coordinate Transformations (http://www.epsg-registry.org/) 7.1.2 Vector data and shapefiles Reading: https://bookdown.org/robinlovelace/geocompr/spatial-data-operations.html#spatial-operations-on-vector-data 7.1.3 Raster data Reading: https://bookdown.org/robinlovelace/geocompr/spatial-data-operations.html#spatial-operations-on-raster-data 7.1.4 Backends 7.1.4.1 GDAL GDAL: Geospatial Data Abstraction Library https://en.wikipedia.org/wiki/GDAL#Software_using_GDAL/OGR 7.1.4.2 GEOS GEOS: Geometry Engine - Open Source https://en.wikipedia.org/wiki/JTS_Topology_Suite#Applications_Using_GEOS 7.1.5 R as a GIS: packages https://cran.r-project.org/web/packages/sp/ 7.1.5.1 rgdal rgdal: Bindings for the ‘Geospatial’ Data Abstraction Library Provides bindings to the ‘Geospatial’ Data Abstraction Library (‘GDAL’) (&gt;= 1.6.3) and access to projection/transformation operations from the ‘PROJ.4’ library. The ‘GDAL’ and ‘PROJ.4’ libraries are external to the package, and, when installing the package from source, must be correctly installed first. Both ‘GDAL’ raster and ‘OGR’ vector map data can be imported into R, and ‘GDAL’ raster data and ‘OGR’ vector data exported. Use is made of classes defined in the ‘sp’ package. Windows and Mac Intel OS X binaries (including ‘GDAL’, ‘PROJ.4’ and ‘Expat’) are provided on ‘CRAN’. https://cran.r-project.org/web/packages/rgdal/ 7.1.5.2 rgeos rgeos: Interface to Geometry Engine - Open Source (‘GEOS’) Interface to Geometry Engine - Open Source (‘GEOS’) using the C ‘API’ for topology operations on geometries. The ‘GEOS’ library is external to the package, and, when installing the package from source, must be correctly installed first. Windows and Mac Intel OS X binaries are provided on ‘CRAN’. https://cran.r-project.org/web/packages/rgeos/ 7.1.5.3 proj4 proj4: A simple interface to the PROJ.4 cartographic projections library A simple interface to lat/long projection and datum transformation of the PROJ.4 cartographic projections library. It allows transformation of geographic coordinates from one projection and/or datum to another. https://cran.r-project.org/web/packages/proj4/ 7.1.5.4 sf sf: Simple Features for R Support for simple features, a standardized way to encode spatial vector data. Binds to GDAL for reading and writing data, to GEOS for geometrical operations, and to Proj.4 for projection conversions and datum transformations. https://cran.r-project.org/web/packages/sf/ 7.1.5.5 sp sp: Classes and Methods for Spatial Data Classes and methods for spatial data; the classes document where the spatial location information resides, for 2D or 3D data. Utility functions are provided, e.g. for plotting data as maps, spatial selection, as well as methods for retrieving coordinates, for subsetting, print, summary, etc. https://cran.r-project.org/web/packages/sp/ 7.1.5.6 ggmap ggmap: Spatial Visualization with ggplot2 A collection of functions to visualize spatial data and models on top of static maps from various online sources (e.g Google Maps and Stamen Maps). It includes tools common to those tasks, including functions for geolocation and routing. https://cran.r-project.org/web/packages/ggmap/ 7.1.5.7 leaflet leaflet: Create Interactive Web Maps with the JavaScript ‘Leaflet’ Library Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package. These maps can be used directly from the R console, from ‘RStudio’, in Shiny apps and R Markdown documents. https://cran.r-project.org/web/packages/leaflet/ https://rstudio.github.io/leaflet/ 7.1.5.8 Pythonic world https://pypi.python.org/pypi/GDAL https://pypi.python.org/pypi/geos 7.1.6 Example US: https://www.youtube.com/watch?v=qu6TiwvjPLs Europe: https://www.youtube.com/watch?v=TYqsa5EvFe4 Power plants and air pollution monitors https://github.com/datasciencelabs/data/blob/master/powerplants.csv https://github.com/datasciencelabs/data/blob/master/pm25.csv power_plants &lt;- read.csv(&quot;https://raw.githubusercontent.com/datasciencelabs/data/master/powerplants.csv&quot;)[, -1] names(power_plants)[4] &lt;- &quot;so2&quot; pm25 &lt;- read.csv(&quot;https://raw.githubusercontent.com/datasciencelabs/data/master/pm25.csv&quot;)[, -1] names(pm25)[4] &lt;- &quot;pm&quot; library(ggmap) ## Loading required package: ggplot2 map &lt;- get_map(&quot;US&quot;, zoom = 4) ## Map from URL : http://maps.googleapis.com/maps/api/staticmap?center=US&amp;zoom=4&amp;size=640x640&amp;scale=2&amp;maptype=terrain&amp;language=en-EN&amp;sensor=false ## Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=US&amp;sensor=false p &lt;- ggmap(map) + geom_point(data = power_plants, aes(x = lon1, y = lat1, alpha = so2), size = 5) + geom_point(data = pm25, aes(x = lon2, y = lat2), color = &quot;red&quot;) p ## Warning: Removed 1 rows containing missing values (geom_point). ## Warning: Removed 37 rows containing missing values (geom_point). 7.1.6.1 Spatial objects power_plants and pm25 are data frames. ggmap “guessed” how they should be displayed. class(power_plants) ## [1] &quot;data.frame&quot; class(pm25) ## [1] &quot;data.frame&quot; We can convert data frames into spatial objects. plot is still “guessing” the CRS. library(sp) spm25 &lt;- pm25 coordinates(spm25) &lt;- ~lon2 + lat2 class(spm25) ## [1] &quot;SpatialPointsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; plot(spm25) library(proj4) proj4string(spm25) ## [1] NA proj4string(spm25) &lt;- CRS(&quot;+init=epsg:4269&quot;) # for most federal agencies NAD83 (EPSG:4269) proj4string(spm25) ## [1] &quot;+init=epsg:4269 +proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; plot(spm25) head(spm25) ## coordinates id2 pm ## 1 (-86.815, 33.55306) 01073-0023 26.44084 ## 2 (-86.92417, 33.49972) 01073-2003 21.65607 ## 3 (-86.77944, 33.58556) 01073-6003 28.02234 ## 4 (-86.79639, 33.56528) 01073-6004 29.56994 ## 5 (-88.08753, 30.76994) 01097-0003 18.82726 ## 6 (-149.8246, 61.20586) 02020-0018 14.00546 head(spTransform(spm25, CRS(&quot;+init=epsg:3857&quot;)), 3) # Mercator: Google map ## coordinates id2 pm ## 1 (-9664202, 3968945) 01073-0023 26.44084 ## 2 (-9676354, 3961823) 01073-2003 21.65607 ## 3 (-9660244, 3973287) 01073-6003 28.02234 plot(spTransform(spm25, CRS(&quot;+init=epsg:3857&quot;))) 7.1.6.2 Spatial operations spm25[1, ] ## coordinates id2 pm ## 1 (-86.815, 33.55306) 01073-0023 26.44084 spm25[2, ] ## coordinates id2 pm ## 2 (-86.92417, 33.49972) 01073-2003 21.65607 p1 &lt;- spm25[1, ] p2 &lt;- spm25[2, ] spDistsN1(p1, p2, longlat=TRUE) # longlat TRUE to get km ## [1] 11.74021 # m &lt;- spDists(p1, spm25, longlat=TRUE) # to get all pairwise distances Let’s check: http://www.tytai.com/gmap/distance/. library(spdep) ## Warning: package &#39;spdep&#39; was built under R version 3.4.3 ## Loading required package: Matrix ## Loading required package: spData ## Warning: package &#39;spData&#39; was built under R version 3.4.3 ## ## Attaching package: &#39;spData&#39; ## The following objects are masked _by_ &#39;.GlobalEnv&#39;: ## ## x, y kn &lt;- knn2nb(knearneigh(spm25, k = 5)) ## Warning in knearneigh(spm25, k = 5): knearneigh: identical points found kn[1] # 3 4 304 306 308 ## [[1]] ## [1] 3 4 304 306 308 pm25[unlist(kn[1]), ] ## id2 lat2 lon2 pm ## 3 01073-6003 33.58556 -86.77944 28.02234 ## 4 01073-6004 33.56528 -86.79639 29.56994 ## 304 01073-0023 33.55306 -86.81500 21.98295 ## 306 01073-6002 33.57833 -86.77389 12.35746 ## 308 01073-6004 33.56528 -86.79639 28.51129 7.1.6.3 Polygons In which states are the PM2.5 monitors? library(tigris) library(raster) s &lt;- states(cb = TRUE, year = 2010) shapefile(s, &quot;~/Dropbox/Data17/census/census.shp&quot;) library(raster) s &lt;- shapefile(&quot;~/Dropbox/Data17/census/census.shp&quot;) sort(unique(s@data$NAME)) ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; ## [4] &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; ## [7] &quot;Connecticut&quot; &quot;Delaware&quot; &quot;District of Columbia&quot; ## [10] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; ## [13] &quot;Idaho&quot; &quot;Illinois&quot; &quot;Indiana&quot; ## [16] &quot;Iowa&quot; &quot;Kansas&quot; &quot;Kentucky&quot; ## [19] &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; ## [22] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; ## [25] &quot;Mississippi&quot; &quot;Missouri&quot; &quot;Montana&quot; ## [28] &quot;Nebraska&quot; &quot;Nevada&quot; &quot;New Hampshire&quot; ## [31] &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; ## [34] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; ## [37] &quot;Oklahoma&quot; &quot;Oregon&quot; &quot;Pennsylvania&quot; ## [40] &quot;Puerto Rico&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; ## [43] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; ## [46] &quot;Utah&quot; &quot;Vermont&quot; &quot;Virginia&quot; ## [49] &quot;Washington&quot; &quot;West Virginia&quot; &quot;Wisconsin&quot; ## [52] &quot;Wyoming&quot; s &lt;- subset(s, ! NAME %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;)) plot(s) proj4string(s) ## [1] &quot;+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; proj4string(spm25) ## [1] &quot;+init=epsg:4269 +proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; spm25proj &lt;- spTransform(spm25, proj4string(s)) proj4string(spm25proj) ## [1] &quot;+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; o &lt;- over(spm25proj, s) odf &lt;- data.frame(spm25@data , o) head(odf) ## id2 pm GEO_ID STATE NAME LSAD CENSUSAREA ## 1 01073-0023 26.44084 0400000US01 01 Alabama &lt;NA&gt; 50645.33 ## 2 01073-2003 21.65607 0400000US01 01 Alabama &lt;NA&gt; 50645.33 ## 3 01073-6003 28.02234 0400000US01 01 Alabama &lt;NA&gt; 50645.33 ## 4 01073-6004 29.56994 0400000US01 01 Alabama &lt;NA&gt; 50645.33 ## 5 01097-0003 18.82726 0400000US01 01 Alabama &lt;NA&gt; 50645.33 ## 6 02020-0018 14.00546 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA 7.1.6.4 Rasters Let’s get temperature data. library(raster) temperatures &lt;- getData(&quot;worldclim&quot;, var = &quot;tmean&quot;, res = 10) tmp_aug &lt;- temperatures[[8]] # select August plot(tmp_aug) tmp_aug_state &lt;- extract(tmp_aug, s, fun = mean, na.rm = TRUE, df = TRUE) ## Warning in .local(x, y, ...): Transforming SpatialPolygons to the CRS of ## the Raster head(cbind(s@data$NAME, 0.1 * tmp_aug_state)) # 0.1 to get back to C ## s@data$NAME ID tmean8 ## 1 Maine 0.1 17.50541 ## 2 Massachusetts 0.2 20.11750 ## 3 Michigan 0.3 18.97676 ## 4 Montana 0.4 17.77345 ## 5 Nevada 0.5 20.37820 ## 6 New Jersey 0.6 22.40779 # tmp_aug_pp &lt;- extract(tmp_aug, spm25, fun = mean, na.rm = TRUE, df = TRUE) 7.1.7 From sp to sf Reading: https://github.com/r-spatial/sf/wiki/migrating 7.2 Principles of visualization Guest lecture (James Honaker) References "],
["references-1.html", "References", " References "]
]
